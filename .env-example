
# =============================================================================
# FRAME CODE CLI - Environment Variables Configuration
# Copie este arquivo para .env e ajuste conforme necessário
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Configuration
# -----------------------------------------------------------------------------
# Chave de API do seu provedor (Obrigatório)
LLM_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Provedor do modelo (openai, anthropic, openai-compatible)
LLM_PROVIDER=openai-compatible

# Modelo padrão a ser utilizado
LLM_DEFAULT_MODEL=gpt-4o-mini

# URL Base para provedores compatíveis (Ex: LM Studio, LocalAI, vLLM)
LLM_BASE_URL=http://localhost:1234/v1

# Token limits
LLM_MAX_OUTPUT_TOKENS=4096   # Máximo de tokens na resposta (output)
LLM_MAX_TOKENS=128000        # Máximo de tokens de contexto (janela de memória)
LLM_TEMPERATURE=0.7          # Criatividade (0.0 a 1.0)

# -----------------------------------------------------------------------------
# Vision / Multimodal (imagens)
# -----------------------------------------------------------------------------
# Se false, enviar imagem para o modelo deve falhar com erro explĆ­cito no SDK.
LLM_SUPPORTS_VISION=false

# Config separada (opcional) para modelos/provider de visĆ£o.
# Se vazio, o code-cli usa os valores de LLM_* acima.
LLM_VISION_PROVIDER=
LLM_VISION_API_KEY=
LLM_VISION_BASE_URL=
LLM_VISION_MODEL=


# -----------------------------------------------------------------------------
# Memory Compression Settings
# Gerencia a compressão automática do histórico quando a janela de contexto enche
# -----------------------------------------------------------------------------
COMPRESSION_ENABLED=true
COMPRESSION_THRESHOLD=0.8      # % do contexto usado para disparar compressão (0.8 = 80%)
COMPRESSION_MAX_COUNT=5        # Quantas compressões manter na memória
COMPRESSION_MAX_TOKENS=1500    # Tamanho máximo do resumo comprimido
COMPRESSION_MODEL=gpt-4o-mini  # Modelo usado para comprimir (se diferente do principal)
COMPRESSION_LOGGING=true       # Logar detalhes da compressão
COMPRESSION_PERSIST=true       # Salvar compressões entre sessões


# -----------------------------------------------------------------------------
# Tools & Agent Configuration
# -----------------------------------------------------------------------------
# Habilita suporte a MCP (Model Context Protocol) para tools externas
MCP_TOOLS_ENABLED=true

# Modo padrão do agente (interactive ou autonomous)
AGENT_MODE=interactive

# Mostra logs das ferramentas no terminal principal (true/false)
SHOW_TOOL_LOGS_INLINE=false


# -----------------------------------------------------------------------------
# Qdrant MCP Server Configuration
# -----------------------------------------------------------------------------
# Configurações para o servidor de busca vetorial Qdrant via MCP

# ID do servidor MCP (opcional, padrão: qdrant-official)
QDRANT_MCP_ID=qdrant-official

# Namespace para as ferramentas Qdrant (opcional, padrão: qdrant)
QDRANT_MCP_NAMESPACE=qdrant

# URL do servidor Qdrant (padrão: http://qdrant:6333 para Docker, use http://localhost:7333 para acesso local)
QDRANT_URL=http://localhost:7333


# -----------------------------------------------------------------------------
# Neo4j MCP Server Configuration
# -----------------------------------------------------------------------------
# Configurações para o servidor de banco de dados Neo4j via MCP

# ID do servidor MCP (opcional, padrão: neo4j-official)
NEO4J_MCP_ID=neo4j-official

# Namespace para as ferramentas Neo4j (opcional, padrão: neo4j)
NEO4J_MCP_NAMESPACE=neo4j

# API Key do Qdrant (opcional, para instâncias com autenticação)
QDRANT_API_KEY=

# Nome da coleção padrão (opcional, padrão: default-collection)
QDRANT_COLLECTION_NAME=default-collection

# Modelo de embedding (opcional, padrão: sentence-transformers/all-MiniLM-L6-v2)
QDRANT_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2


# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
# Níveis de log disponíveis:
# - DEBUG=true → mostra DEBUG, INFO, WARN, ERROR (mais detalhado)
# - INFO=true  → mostra INFO, WARN, ERROR (sem DEBUG)
# - padrão    → mostra apenas WARN, ERROR (silencioso)
DEBUG=false
INFO=false

# -----------------------------------------------------------------------------
# Console Encoding (Windows)
# -----------------------------------------------------------------------------
# Para exibir corretamente caracteres especiais (acentos, emojis, etc.) no
# Windows, configure o console para UTF-8:
#
# No Git Bash / MinGW:
#   export LANG=pt_BR.UTF-8
#   export LC_ALL=pt_BR.UTF-8
#
# No PowerShell / CMD:
#   chcp 65001
#
# Ou adicione ao seu ~/.bashrc:
#   export LANG=pt_BR.UTF-8
#   export LC_ALL=pt_BR.UTF-8
#   alias npm='LANG=pt_BR.UTF-8 npm'



# -----------------------------------------------------------------------------
# Chrome DevTools MCP Server Configuration
# -----------------------------------------------------------------------------
# Configurações para o servidor de automação do Chrome via MCP

# ID do servidor MCP (opcional, padrão: chrome-devtools)
CHROME_MCP_ID=chrome-devtools

# Nome do container Docker do Chrome MCP (opcional, padrão: chrome-devtools-mcp-server)
CHROME_MCP_CONTAINER=chrome-devtools-mcp-server

# Namespace para as ferramentas Chrome (opcional, padrão: chrome)
CHROME_MCP_NAMESPACE=chrome

# Porta do Chrome DevTools Protocol (opcional, padrão: 9222)
CHROME_DEBUG_PORT=9222
